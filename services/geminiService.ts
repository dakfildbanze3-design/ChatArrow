
import { Message, GroundingUrl, AppSettings } from "../types";
import { supabase } from "./supabaseService";
import { GoogleGenAI, GenerateContentResponse } from "@google/genai";

export class GeminiService {
  private DEFAULT_INSTRUCTION = `Voc√™ √© um Consultor Estrat√©gico e Especialista T√©cnico de elite. Sua miss√£o √© entregar respostas que resolvam problemas reais com autoridade, clareza e profundidade estrat√©gica.

REGRA CR√çTICA DE IDENTIDADE:
- NUNCA se apresente.
- NUNCA diga "Eu sou uma intelig√™ncia artificial", "Sou um modelo de linguagem", "Ol√°, eu sou..." ou qualquer varia√ß√£o disso.
- V√° direto ao ponto e responda a pergunta do usu√°rio imediatamente, sem introdu√ß√µes sobre quem ou o que voc√™ √©.
- Aja de forma invis√≠vel, fornecendo apenas a informa√ß√£o solicitada.

REGRAS DE TOM E COMPORTAMENTO:
- Responda sempre com extrema formalidade, responsabilidade e respeito.
- Demonstre considera√ß√£o e empatia pelo usu√°rio.
- Utilize um vocabul√°rio respons√°vel, polido e profissional em todas as intera√ß√µes.
- Evite g√≠rias, express√µes coloquiais ou tom excessivamente √≠ntimo.

REGRAS DE FORMATA√á√ÉO E ORGANIZA√á√ÉO (OBRIGAT√ìRIO):
Voc√™ deve formatar TODAS as suas respostas seguindo EXATAMENTE esta estrutura visual de "estrofes":

1. Comece cada se√ß√£o com um T√≠tulo em negrito e um emoji (ex: **üí≥ O que aconteceu**)
2. Se houver uma cita√ß√£o ou mensagem de erro, use blockquote (ex: > "Mensagem de erro")
3. Use uma frase curta de introdu√ß√£o (ex: Quer dizer:)
4. Use uma lista com marcadores (bullet points) ou n√∫meros para os detalhes. Cada item da lista deve ser curto.
5. Adicione 1 ou 2 frases curtas de conclus√£o ap√≥s a lista.
6. OBRIGAT√ìRIO: Separe CADA se√ß√£o/estrofe com uma linha divis√≥ria horizontal (---).

Exemplo de Estrutura Esperada:
**ü§ñ T√≠tulo da Se√ß√£o**

> "Cita√ß√£o ou foco principal se aplic√°vel"

Frase introdut√≥ria:
- Ponto 1 curto e direto
- Ponto 2 curto e direto
- Ponto 3 curto e direto

Conclus√£o curta e direta.

---

**üî• Pr√≥ximo T√≠tulo**
...

NUNCA crie blocos de texto densos. Mantenha muito espa√ßo em branco e leitura escane√°vel.`;

  private ai: GoogleGenAI;

  constructor() {
    this.ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY || '' });
  }

  private getSystemInstruction(settings?: AppSettings, customInstruction?: string): string {
    if (customInstruction) return customInstruction;
    
    const base = this.DEFAULT_INSTRUCTION;
    if (!settings) return base;

    const { ai } = settings;
    
    return `${base}

CONFIGURA√á√ïES ADICIONAIS DO USU√ÅRIO:
- Idioma: ${ai.language}
- Personalidade Base: ${ai.personality}
- Tom de Voz: ${ai.style}
- Prefer√™ncia de Tamanho: ${ai.length} (Priorize a profundidade solicitada nas diretrizes principais acima).`;
  }

  async sendMessageStream(
    message: string, 
    history: Message[], 
    currentImages?: string[], 
    customInstruction?: string, 
    settings?: AppSettings,
    onChunk?: (text: string) => void
  ): Promise<{ text: string; images?: string[]; groundingUrls?: GroundingUrl[]; usage?: any; model?: string }> {
    try {
      // 1. Otimiza√ß√£o de Hist√≥rico: 
      // Se o hist√≥rico for muito longo (> 10 mensagens), resumimos as mensagens antigas
      let processedHistory = [...history];
      if (processedHistory.length > 10) {
        const toSummarize = processedHistory.slice(0, -5);
        const recent = processedHistory.slice(-5);
        
        // Simula√ß√£o de resumo para economizar tokens e tempo
        // Em um cen√°rio real, poder√≠amos chamar o modelo para resumir, 
        // mas para velocidade instant√¢nea, apenas truncamos ou usamos um resumo est√°tico
        const summary = `[Resumo do contexto anterior: O usu√°rio e a IA discutiram sobre ${toSummarize.length} t√≥picos anteriores.]`;
        processedHistory = [{ id: 'summary', role: 'model', text: summary, timestamp: new Date() }, ...recent];
      } else {
        processedHistory = processedHistory.slice(-5); // Mant√©m apenas as √∫ltimas 5 para lat√™ncia m√≠nima
      }

      const contents = processedHistory.map(msg => {
        const parts: any[] = [{ text: msg.text || " " }];
        
        if (msg.images && msg.images.length > 0) {
          msg.images.forEach(img => {
            if (img.startsWith('data:')) {
              const [header, data] = img.split(';base64,');
              const mimeType = header.split(':')[1];
              parts.push({
                inlineData: { mimeType, data }
              });
            }
          });
        }
        
        return {
          role: msg.role === 'user' ? 'user' : 'model',
          parts
        };
      });

      // Adiciona a mensagem atual
      const currentParts: any[] = [{ text: message || " " }];
      if (currentImages && currentImages.length > 0) {
        currentImages.forEach(img => {
          if (img.startsWith('data:')) {
            const [header, data] = img.split(';base64,');
            const mimeType = header.split(':')[1];
            currentParts.push({
              inlineData: { mimeType, data }
            });
          }
        });
      }

      contents.push({
        role: 'user',
        parts: currentParts
      });

      // 2. Modelo Otimizado para Velocidade
      const modelName = settings?.model.mode === 'Preciso' ? 'gemini-3.1-pro-preview' : 'gemini-3-flash-preview';

      const responseStream = await this.ai.models.generateContentStream({
        model: modelName,
        contents: contents,
        config: {
          systemInstruction: this.getSystemInstruction(settings, customInstruction),
          temperature: settings?.model.mode === 'Criativo' ? 0.8 : 0.2,
          maxOutputTokens: 2048, // Aumentado para permitir respostas detalhadas e profundas
        },
      });

      let fullText = "";
      let usage: any = null;

      for await (const chunk of responseStream) {
        const chunkText = chunk.text || "";
        fullText += chunkText;
        if (onChunk) onChunk(fullText);
        if (chunk.usageMetadata) usage = chunk.usageMetadata;
      }

      return { 
        text: fullText, 
        usage,
        model: modelName
      };
    } catch (error) {
      console.error("Gemini Stream Error:", error);
      throw new Error("Falha na comunica√ß√£o em tempo real.");
    }
  }

  async generateTitle(firstMessage: string): Promise<string> {
    try {
      const response = await this.ai.models.generateContent({
        model: 'gemini-3-flash-preview',
        contents: [{ role: 'user', parts: [{ text: `Gere um t√≠tulo curto (m√°ximo 4 palavras) para: "${firstMessage}". Responda APENAS com o t√≠tulo.` }] }],
        config: { temperature: 0.5 }
      });
      return response.text?.trim() || "Nova Conversa";
    } catch (error) {
      return firstMessage.length > 30 ? firstMessage.substring(0, 27) + "..." : firstMessage;
    }
  }
}

export const geminiService = new GeminiService();
